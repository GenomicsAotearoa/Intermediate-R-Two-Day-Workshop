[
  {
    "objectID": "day_1_overview.html",
    "href": "day_1_overview.html",
    "title": "Day One Overview",
    "section": "",
    "text": "The focus of Day 1 is string manipulation and managing data across files\nString manipulation will involve:\n\nRegular expressions (regex)\nThe functions str_detect, str_subset, str_replace\n\nManaging data across files will involve:\n\nThe join family of functions",
    "crumbs": [
      "Day One"
    ]
  },
  {
    "objectID": "episode_2.html",
    "href": "episode_2.html",
    "title": "Episode 2 - String manipulation",
    "section": "",
    "text": "It’s a good day when the data you receive is in the exact format you need for your analysis. Much more frequently, you will need to carry out some form of data tidying. Data tidying is a general term to describe things like renaming columns, removing rows with missing data, or combining data from different files. This episode will focus on string manipulation - combining, removing, or renaming strings of characters. The next episode will focus on integrating data from different files."
  },
  {
    "objectID": "episode_2.html#an-example-of-string-manipulation",
    "href": "episode_2.html#an-example-of-string-manipulation",
    "title": "Episode 2 - String manipulation",
    "section": "An example of string manipulation",
    "text": "An example of string manipulation\nLet’s look at some example data to demonstrate what we are going to try and do today:\n\n\n\nTaxon column in the tax object\n\n\nIn this file individual rows represent sequences which have been assigned a taxonomic lineage. The Taxon column is not particularly human readable right now, but let’s say we wanted to modify this into something shorter and cleaner that we could then use as figure labels (e.g., convert row 1 to read “Bacteria, Cyanobacteria”).\nIf this was a single row, we would probably do this in excel - but since there are more than 4,000 rows, we want to automate this process. This type of string manipulation is very powerful and is something you can expect to regularly carry out in your data analysis career."
  },
  {
    "objectID": "episode_2.html#regular-expressions",
    "href": "episode_2.html#regular-expressions",
    "title": "Episode 2 - String manipulation",
    "section": "Regular expressions",
    "text": "Regular expressions\nBefore we look at the functions we will use for string manipulation we need to introduce regular expressions (called regex or regexp). Regex are a set of rules and keys that we can use to describe patterns that we want to match or manipulate. One example of this type of rule or key is the concept of the wildcard - a symbol that matches to any value (e.g., if I have files labelled “Sample_A”, “Sample_B”, “Sample_C”, then “Sample_wildcard” would match to all of the samples). Regex has a whole series of symbols that can be used in combination to define patterns. Here is a more complex example:\n[0-9]+.*_(log|chk)\\.txt\nThe regex above is probaby nonsensical to you. It might help to split it into the rules (symbols like the wildcard that are meaning something in the regex language) and the arguments (the patterns we are trying to match to):\n[0-9]+.**_(log|chk)\\.txt*\nNow we can read the arguments from left to right:\n\n[0-9] : The square brackets are symbols that specify a range - in this case, any number from 0 - 9. We could also use [a-e] to match any letters from a-e, or [DGT] to match any of the letters D, G, or T. You can think of values inside of square brackets as being very limited wildcards which will only replace a certain characters.\n\n: The “+” is a modifier, which can be read as “match the preceeding pattern one or more times”. The addition of the “+” symbol changes the regex from matching 0, or 1, or 5 (single digits only), to matching 0, or 10, or 55555593939483829.\n\n. : The “.” is the regex wildcard. This matches any character. An example use of this is when you have a mix of column names like “Sample_1”, “Sample.1”,“Sample-1”.\n* : The “*” is another modifier, but this time it matches the preceeding pattern zero or more times.\n_ : This is an _ (i.e., we know that the string has something_something).\n(log|chk) : There are two separate rules here. The () indicates that the patterns inside must be interpreted as words, while the “|” means “or”. Together, this means “match to the word log or the word chk”.\n\\. : The “\\” is known as “escape”, and we use it when we need to specify a “.” or an “*” (so that they are interpreted as characters, rather than rules that modify the regex).\ntxt : A set of characters.\n\nExercise: Can you read the regex? Use regular language to describe the type of pattern we are matching. Give one example of a pattern that the regex would match to, then give a different pattern.\n\n\n\n\n\n\nReading the regex\n\n\n\n\n\nAll together, this regex is looking for a file that starts with one or more digits, maybe (or maybe not) some other type of characters, an underscore, the word log or chk, ending in .txt.\nExamples: 12345_log.txt, 001A_log.txt, 104++_chk.txt\n\n\n\nExercise: Write a regex pattern that will match the following cases:\n\nYou have received samples with two different formats: “Sample1_A” or “SampleA_1”. There are five samples, each with an A or B type. Write a regex pattern that will match with all possible sample names.\n\nNow imagine that there are many more samples, but we only want to match with the first five.\n\n\n\n\n\n\n\n\nReading the regex\n\n\n\n\n\n\nSample._. would capture all possible combinations with single characters (e.g., samples 0-9), and would also pull out anything with errors (e.g., Sample?_+). Sample[1-5|AB]_[1-5|AB] would capture only samples 1-5."
  },
  {
    "objectID": "episode_2.html#wrangling-taxonomy",
    "href": "episode_2.html#wrangling-taxonomy",
    "title": "Episode 2 - String manipulation",
    "section": "Wrangling taxonomy",
    "text": "Wrangling taxonomy\nHere we will work with our example files with the aim to take the current taxonomy data and convert it into something more human readable.\n\nSetup R environment\n\nsource(\"setup.R\")\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\nObtaining microbial taxonomy from DNA sequences\nA major aim of microbial ecology is the identification of populations across an environment. We do that by sequencing the amplicon of the 16S small subunit ribosomal RNA gene, the standard taxonomic marker. Then, sequences are clustered based on sequence similarity (to reduce redundancy and improve computational efficiency) and then assigned a taxonomic lineage using a classifier that compares our sequence data with those in a reference database (popular options are SILVA and Greengenes 2). Depending on how similar and well-represented the sampled sequences are to those in the database, our sequences will be assigned names and ranks ranging from domain to species.\n\n\nInspecting taxonomy\nFirst, we will look at our current taxonomy field and familiarise ourselves with the data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntax$Taxon %&gt;% head()\n\n[1] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                                   \n[2] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                                   \n[3] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                                   \n[4] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                                   \n[5] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Steroidobacterales; f__Woeseiaceae; g__Woeseia\"                          \n[6] \"d__Bacteria; p__Proteobacteria; c__Alphaproteobacteria; o__Rickettsiales; f__Mitochondria; g__Mitochondria; s__uncultured_bacterium\"\n\ntax$Taxon %&gt;% tail()\n\n[1] \"d__Bacteria; p__Bacteroidota; c__Bacteroidia; o__Bacteroidales; f__Marinilabiliaceae; g__Carboxylicivirga\"                   \n[2] \"d__Bacteria; p__Acidobacteriota; c__Thermoanaerobaculia; o__Thermoanaerobaculales; f__Thermoanaerobaculaceae; g__Subgroup_10\"\n[3] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                            \n[4] \"d__Bacteria; p__Acidobacteriota; c__Thermoanaerobaculia; o__Thermoanaerobaculales; f__Thermoanaerobaculaceae; g__Subgroup_10\"\n[5] \"d__Bacteria; p__Cyanobacteria; c__Cyanobacteriia; o__Chloroplast; f__Chloroplast; g__Chloroplast\"                            \n[6] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Steroidobacterales; f__Woeseiaceae; g__Woeseia\"                   \n\n\nWe can see that for each row we have a sequence which has a classification going down through one or more ranks (domain, phylum, class etc.,). Each rank begins with a single-letter prefix followed by “__“. Ranks are separated by a semicolon (”;“). Importantly, ranks are unevenly assigned - some are identified down to the species level, while some are only classified to the level of phylum.\n\n\nPattern detection and extraction with the str_ family of functions\nNow that we have an understanding of regex and we know what our data looks like, we can use functions that will take regex and match to patterns within our data. There is a family of functions that all begin with str_ (e.g., such as str_detect, str_count, str_subset) which will can be used to do many different tasks.\nWe will use functions to answer two questions:\n\nHow well characterised are our sequences? i.e., what proportion of our sequences are classified down to the level of species?\nDid we manage to retrieve biologically important taxa? (for this example, we will select some taxa for further investigation)\n\n1. How well characterised are our sequences?\nHow many sequences were classified at each taxonomic rank (species, genus, family, order, class, phylum)? If there are large numbers of sequences that were only identified at higher taxonomic ranks, the system we are studying may harbour lots of novel microbial populations.\nTo answer this question we will start with the str_detect() function. str_detect() takes a regex pattern as input and will return a set of TRUE/FALSE values for each element (in our case, each sequence that has a taxonomic classification) depending on whether the pattern is present/absent.\n\nstr_detect(tax$Taxon, \"s__\") %&gt;%    # s__ indicates a species classification\n    sum()       # sum() will count the number of \"TRUE\" results\n\n[1] 2040\n\n\nThis tells us that of the 4,220 sequences with a taxonomic classification, 2040 were classified to the level of species (“s__“).\nMost of the functions in the str_ family follow the same format:\nstr_name(vector, regex pattern, additional arguments)\nRemember that str_detect() works by testing each element (here, each individual row from tax in the form of a vector). If we wanted to count how many times a string appears within each element, we can use str_count():\n\nstr_count(tax$Taxon, \"__\") %&gt;%\n    head(n = 20)    # Shows the number of classification levels for the first 20 elements\n\n [1] 6 6 6 6 6 7 6 6 7 6 6 5 7 6 6 6 6 7 6 7\n\nstr_count(tax$Taxon, \"__\") %&gt;%\n    hist(main = \"Frequency of classification depth\")    \n\n\n\n\n\n\n\n\nWe can see that the majority of the sequences are classified to the level of species or genus.\n2. Did we retrieve biologically important taxa?\nFor this episode we will focus on species that contribute to the removal of nitrogen from the ecosystem (the act of denitrification). This is usually performed by prokaryotes spanning the Bacterial and Archaeal domains. Their metabolic activity ensures that excess nitrogen is removed in gaseous form and thus prevents eutrophication. The starting substrate for denitrification is nitrate. Thus, reduced nitrogen must first be oxidised via nitrification. Two communities are involved in the conversion from reduced to oxidised nitrogen:\n\nAmmonia oxidisers (usually has the prefix “Nitroso” in their taxonomy)\nNitrite oxidisers (usually has the prefix “Nitro” in their taxonomy)\n\nWe will use the str_subset() function to pull out all rows with “Nitro” in their taxonomy, then we will use the str_replace() function and a complex regex statement to replace the long and complex taxonomy name with something clear and easy to read.\n\nnitro &lt;- str_subset(tax$Taxon, \"__Nitro\")\n\nnitro %&gt;% length()\n\n[1] 102\n\nnitro %&gt;% head()\n\n[1] \"d__Bacteria; p__Nitrospirota; c__Nitrospiria; o__Nitrospirales; f__Nitrospiraceae; g__Nitrospira; s__uncultured_Cytophaga\"            \n[2] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Nitrosococcales; f__Nitrosococcaceae; g__SZB85; s__uncultured_bacterium\"   \n[3] \"d__Archaea; p__Crenarchaeota; c__Nitrososphaeria; o__Nitrosopumilales; f__Nitrosopumilaceae; g__Candidatus_Nitrosopumilus\"            \n[4] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Nitrosococcales; f__Nitrosococcaceae; g__SZB85; s__uncultured_bacterium\"   \n[5] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Nitrosococcales; f__Nitrosococcaceae; g__SZB85; s__uncultured_bacterium\"   \n[6] \"d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Nitrosococcales; f__Nitrosococcaceae; g__FS142-36B-02; s__uncultured_gamma\"\n\n\nThere are 102 sequences assigned to a taxonomy with “Nitro”.\nNow we will use the str_replace() function to replace the long taxonomic classification with only the name of the domain and the deepest classification containing the word “Nitro”.\nTo understand str_replace() we are going to need to introduce two new rules (in addition to the [], “.”, “+”, rules from earlier):\n\n^ : when the caret (“^”) is used inside square brackets, it means “Not this”. Therefore [^;] means “match anything except the semicolon”.\n() and \\1 : when using str_replace we use () to highlight regions we want to replace. Whatever is within the () will be captured and ‘saved’ for later. We can save multiple strings. The “\\1” says “return the first string we saved earlier”.\n\n\nstr_replace(\n  nitro,\n  \"d__([^;]+);.*(Nitro[a-z]+).*\",\n  \"\\\\1, \\\\2\"\n) %&gt;% \n    head()\n\n[1] \"Bacteria, Nitrospira\"       \"Bacteria, Nitrosococcaceae\"\n[3] \"Archaea, Nitrosopumilus\"    \"Bacteria, Nitrosococcaceae\"\n[5] \"Bacteria, Nitrosococcaceae\" \"Bacteria, Nitrosococcaceae\"\n\n\n\nd__([^;]+); looks for the sub-string d__ followed by anything that is not a semicolon [^;]+ more than once. The regex [^&lt;some_pattern&gt;] means to match anything that is NOT . The round brackets () “captures” or “saves” the matches within it for replacement. This is followed by a semicolon (our rank separator) which is not captured but is present in the vector.\n.(Nitro[a-z]+). As we do not know at which rank the first instance of “Nitro” will appear, the regex .* will match anything . more than 0 times *. At the first “Nitro” it encounters, we will also look for any subsequent letters in small case ranging from ‘a’ to ‘z’ as represented by Nitro[a-z]+. Anything after that can be matched but is not captured.\nThe last argument in the function specifies how the replacement string should look like. \\1, \\2 replaces the output with the two patterns we captured separated by a comma and a space. Patterns are captured sequentially and must be referenced in the order which they appear in the original string. Therefore, if we wanted the “Nitro” part to be in front, we would reverse the order to \\2, \\1.\n\nCompare the output with the output from nitro %&gt;% head() to grasp how this works.\nExercise: Do you notice anything that is different from what you expected?\n\n\n\n\n\n\nBeing greedy\n\n\n\n\n\nIt is very reasonable to expect the pattern to return the first match (in our example, that would be “Nitrospirota”). Instead, it has returned Nitrospira - the last match!\nThis is explained by thinking of “.*” as being greedy. It will match as much as possible, while still allowing the rest of the pattern to match. In other words, we get the last possible case of Nitro captured, and all other cases are consumed by the greedy .*."
  },
  {
    "objectID": "day_2_overview.html",
    "href": "day_2_overview.html",
    "title": "Day Two Overview",
    "section": "",
    "text": "The focus of Day 2 is functions and iterations\nFunctions\nIterations",
    "crumbs": [
      "Day Two"
    ]
  },
  {
    "objectID": "episode_1.html",
    "href": "episode_1.html",
    "title": "Episode 1 - Setting Up",
    "section": "",
    "text": "The data that we will be working with today came from sequences generated for this study of nitrogen cycling within prokaryotic communities. Our data is a subset of the sequence data reanalysed using a popular amplicon sequence processing pipeline QIIME2.\nNote that you do not need to be familiar with this type of data to follow along with this workshop. In fact, it is common in data analysis to encounter files and data types that you are not experienced with. In this case it is important to focus on the fundamentals: keeping note of file names, sample/column names, and making yourself familiar with what data is included in each of your files. Below is a brief explanation of the data structure, which will be all you need to proceed.\nThis data consists of 21 sediment samples taken from the Waiwera Estuary (North of Auckland). It was part of a study that looked at how prokaryotic communities and their nitrogen cycling fractions changed along a gradient of mud contents. Alongside sequence data, environmental variables such as mud content (percentage dry weight of clays and silt) and various chemical data such as carbon, nitrogen, sulfur and phosphorus were also measured.\nThis dataset follows a typical microbial ecology study structure, and has three distinct data files:\n\nasv: a count of organisms (here, ASV or amplicon sequence variants) per sample\ntax: describes the taxonomic lineage of ASVs\nenv: sample metadata describing various non-biological measurements obtained from the samples",
    "crumbs": [
      "Day One",
      "Episode 1 - Setting Up"
    ]
  },
  {
    "objectID": "episode_1.html#an-introduction-to-the-data",
    "href": "episode_1.html#an-introduction-to-the-data",
    "title": "Episode 1 - Setting Up",
    "section": "",
    "text": "The data that we will be working with today came from sequences generated for this study of nitrogen cycling within prokaryotic communities. Our data is a subset of the sequence data reanalysed using a popular amplicon sequence processing pipeline QIIME2.\nNote that you do not need to be familiar with this type of data to follow along with this workshop. In fact, it is common in data analysis to encounter files and data types that you are not experienced with. In this case it is important to focus on the fundamentals: keeping note of file names, sample/column names, and making yourself familiar with what data is included in each of your files. Below is a brief explanation of the data structure, which will be all you need to proceed.\nThis data consists of 21 sediment samples taken from the Waiwera Estuary (North of Auckland). It was part of a study that looked at how prokaryotic communities and their nitrogen cycling fractions changed along a gradient of mud contents. Alongside sequence data, environmental variables such as mud content (percentage dry weight of clays and silt) and various chemical data such as carbon, nitrogen, sulfur and phosphorus were also measured.\nThis dataset follows a typical microbial ecology study structure, and has three distinct data files:\n\nasv: a count of organisms (here, ASV or amplicon sequence variants) per sample\ntax: describes the taxonomic lineage of ASVs\nenv: sample metadata describing various non-biological measurements obtained from the samples",
    "crumbs": [
      "Day One",
      "Episode 1 - Setting Up"
    ]
  },
  {
    "objectID": "episode_1.html#load-packages-and-import-data",
    "href": "episode_1.html#load-packages-and-import-data",
    "title": "Episode 1 - Setting Up",
    "section": "Load packages and import data",
    "text": "Load packages and import data\n\n# Load the packages we will be using today\nlibrary(vegan)\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\n\n\n# Import three data files stored remotely\nasv &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/asv_table.tsv\")\nenv &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/env_table.tsv\")\ntax &lt;- read.delim(\"https://raw.githubusercontent.com/GenomicsAotearoa/Intermediate-R/main/tables/taxonomy.tsv\")\n\nasv %&gt;% head()\n\n                             ASVID AS1A1 AS1A2 AS1A3 AS1B1 AS1B2 AS1B3 AS1C1\n1 88c2f5ea8fd2f13cbd43e933049707c9    45    22    37    50    55    51    49\n2 bab6b4a509dd01b93cbbf78e40616533    81    48    79    96    87    94    30\n3 24b484137e882a5c77ea93b4574332e1    70    16    53    86    55    74    11\n4 5d112573567f99604339d5d6b92bdea9    35    14    26    63    45    50    43\n5 864ea29bffe398510cca601c0ce546c2    61    32    37    72    75    61    14\n6 0d808daa4a3a539121a98948ba6caf1f    25    19    23    43    35    41    21\n  AS1C2 AS1C3 AS2A1 AS2A2 AS2A3 AS2B1 AS2B2 AS2B3 AS2C1 AS2C2 AS2C3 S3C1 S3C2\n1    70    54    68    24    80    62    37    39   114   118   122    8   56\n2    30    44    89    52    89    81    34    70    34    38    55    0    9\n3    25    19    90    46    66    43    53    65    22    28    38    0    0\n4    47    44    53    18    41    46    37    43    50    52    55    8   34\n5     0     6    55    49    80    69    56    61    23    12    25    0    6\n6    30    32    20     9    24    16    21    14    67    50    70    3   10\n  S3C3\n1   16\n2    2\n3    4\n4    4\n5    0\n6    0\n\n\n\n Previous Page   Next Page",
    "crumbs": [
      "Day One",
      "Episode 1 - Setting Up"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate R",
    "section": "",
    "text": "In this workshop you will begin to learn intermediate level skills in the R programming language. There are many skills and tools that could be defined as “intermediate level”, and here we will work with just a small number. This workshop will be taught over two mornings from 9am - 1pm.\nDay 1\nDay 2",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Intermediate R",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo fully engage with the material in this workshop, you will need:\nAn Introductory level of R including:\n\nVariable assignment\nMathematical and Boolean operators\nData modes\nImporting data\n\nSome familiarity with the tidyverse dialect of R, as shown here:\n\nmutate() and filter()\ngroup_by() and summarise()\nThe pipe: %&gt;%",
    "crumbs": [
      "Introduction"
    ]
  }
]